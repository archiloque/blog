= Links - 7^th^ May 2023
Julien Kirch
v1.0, 23-05-07
:article_lang: en
:figure-caption!:
:article_description: Build Your Own Database, Abstract Machine Models, Prodigal Techbro, rewrite trap, seven programming ur-languages, UX research reckoning, real-time messaging architecture at Slack


== link:https://build-your-own.org/blog/20230420_byodb_done/[The "`Build Your Own Database`" book is finished]

[quote]
____
Databases are a fascinating topic. They are a foundation of modern
computing. Learning how they work should be an important part of
software engineering education.

As many of today's (2023+) coders do not have a formal CS/SE education,
basic things such as databases, compilers, operating systems, etc. are
often seen as magical black boxes. That's why I started the "`Build Your
Own X`" book series. To learn and teach the basics in a "`from scratch`"
approach, through _succinct_ & condensed books.

There are some important topics that we can learn from database systems:

. _Persistence_. How not to lose or corrupt your data. Recovering from a
crash.
. _Indexing_. Efficiently querying and manipulating your data. (B-tree).
. _Concurrency_. How to handle multiple (large number of) clients. And
transactions.
____

== link:https://dr-knz.net/abstract-machine-models.html[Abstract Machine Models - Also: what Rust got particularly right]

[quote]
____
Ever since 2010, I have studied the "`meta`" of software, by studying (and
thinking about) the continued dialogue between programming language
designers, computer designers, and programmers.

The following constitutes a snapshot of my current thinking.
____

[quote]
____
From there, I focused on the following: "`what's in the mind of
programmers, when they choose one way of doing things over another
that's functionally equivalent?`"

The one thing that was clear from the start, is that most programmers
"`simulate`" the behavior of their program in their mind, to predict how
the program will behave at run-time.

As we've determined above, that simulation does not happen in the
_functional_ model of the programming language.

Meanwhile, I knew from my teaching practice that nobody really
understands hardware computers, and so this mental simulation was also
not happening with a model of a _hardware_ platform. In fact, I've found
that folk would rather not think about hardware at all, and thankfully
so: this made it possible, over and over, to _port software_ from one
hardware platform to another, without rewriting the software.

This meant that all programmers are able to construct a somewhat
_abstract_ model of their computer in their mind, but not so abstract
that it becomes _purely functional_.

That is when I coined the phrase _abstract machine model_, and it became the anchor of my subsequent study.
____

[quote]
____
One thing that bothered me much early on was whether AMMs were truly
distinct from programming languages or the computers that we use.

The question was really: _when a programmer thinks about the run-time behavior of their program, are they only able to formulate their thoughts within the confines of the language they're using to write the program or the computer they're working with?_
____

[quote]
____
In summary, I incrementally developed an understanding that:

* Programmers use AMMs to write software.
* AMMs exist separately from programming languages, and separately from
hardware platforms.
* There is more than one AMM, and AMMs differ in prediction
rules and expressivity.
* An AMM can sometimes be used to program effectively across
multiple languages, but not all.
* An AMM can sometimes be used to program effectively across
multiple hardware computers, but not all.
____

[quote]
____
And so it was interesting to me to wonder: "`when do AMMs appear? When does a programming language designer push for a new AMM, and when can they slip into the shoes of an existing community?`"

While building the table above and studying PL history, I
discovered that language designers come in three groups:

. _machine-first designers_, who start with one or more hardware
platform that's sufficiently different from everything that was done
before that it needs a new AMM, and often a new programming
language to program it.
. _second-language designers_, who assume the existence of some
machine/language ecosystem, adopts it and simply adds new abstractions /
expressivity on top.
. _AMM-first designers_, who are interested to control the way
programmers think first (usually, due to some idea about how this will
result in better software quality), and who merely think about hardware
diversity as an inconvenience that needs to be hidden from programmers.
____

[quote]
____
I am now able to explain that what makes certain
programming problems "`hard`" or "`interesting`" is not related to oddities
in hardware or programming languages, but rather to the way programmers
think about machines, i.e. the properties of their AMMs.

This makes me able to connect related software challenges across
programming language boundaries, or to recognize when similar-looking
programs in different languages have, in fact, extremely
different semantics.

It also makes me able to estimate how much time or effort it will take
me to learn a new technology stack or programming language: if I can
track its ancestry and design principles, I can estimate its conceptual
distance to AMMs I already know.

It also makes me able to estimate whether an already-written program
will work well on a new computer, with or without translation to a
different language or machine instruction set (ISA), depending
on what I know of the AMM that its programmer likely had in
mind when the program was written.
____

== link:https://conversationalist.org/2020/03/05/the-prodigal-techbro/[The Prodigal Techbro]

[quote]
____
The Prodigal Tech Bro is a similar story, about tech executives who experience a sort of religious awakening. They suddenly see their former employers as toxic, and reinvent themselves as experts on taming the tech giants. They were lost and are now found. They are warmly welcomed home to the center of our discourse with invitations to write opeds for major newspapers, for think tank funding, book deals and TED talks. These guys -- and yes, they are all guys -- are generally thoughtful and well-meaning, and I wish them well. But I question why they seize so much attention and are awarded scarce resources, and why they're given not just a second chance, but also the mantle of moral and expert authority.
____

[quote]
____
Today, when the tide of public opinion on Big Tech is finally turning, the brothers (and sisters) who worked hard in the field all those years aren't even invited to the party. No fattened calf for you, my all but unemployable tech activist. The moral hazard is clear; why would anyone do the right thing from the beginning when they can take the money, have their fun, and then, when the wind changes, convert their status and relative wealth into special pleading and a whole new career?
____

== link:https://skamille.medium.com/avoiding-the-rewrite-trap-b1283b8dd39e[Avoiding the rewrite trap]

[quote]
____
Someone has to run and modify the old system while you're writing the new one. But that job sucks, and they're likely to quit before you're done.

You are imagining that your whole team can swarm on the new thing and just knock it out. If you could do the rewrite in a few weeks, maybe. But more likely, you have to keep some people back to keep the old system running, fix bugs, or even add new features to that old system. If those people think that they are on a sinking ship, they are likely to quit, leaving you with a code base that no one wants to support but is still critical to paying the bills. Sure you could rotate the team through supporting the old system, but over time the people who know the old system are likely to leave, and the newcomers will disdain learning the legacy stack.
____

== link:https://madhadron.com/programming/seven_ur_languages.html[The seven programming ur-languages]

[quote]
____
But not all languages have the same set of patterns. The patterns for
looping in C or Python are very different from the patterns of recursion
in Standard ML or Prolog. The way you organize a program in Lisp, where
you name new language constructs, is very different from how you
organize it in APL, where fragments of symbol sequences are both the
definitions of behavior and become the label for that behavior in your
mind.

These distinct collections of fundamentals form various _ur_-languages.
Learning a new language that traces to the same _ur_-language is an easy
shift. Learning one that traces to an unfamiliar _ur_-language requires
significant time and effort and new neural pathways.

I am aware of seven _ur_-languages in software today. I'll name them for
a _type specimen_, the way a species in paleontology is named for a
particular fossil that defines it and then other fossils are compared to
the type specimen to determine their identity. The _ur_-languages are:

* ALGOL
* Lisp
* ML
* Self
* Forth
* APL
* Prolog
____

== link:https://medium.com/onebigthought/the-ux-research-reckoning-is-here-c63710ea4084[The UX research reckoning is here]

[quote]
____
There are three types of work that UX Researchers need to do:

* _Macro-research_ is strategic in nature, business-first, and
future-thinking. It provides concrete frameworks that guide macro
business decisions.
* _Middle-range research_ is focused on user understanding and
product development.
* _Micro-research_ is closer to technical usability, eye
tracking, and detailed interaction development.

_The biggest reason UX Research is facing this reckoning is that we do
way, way too much middle-range research._

Middle-range research is a deadly combination of interesting to
researchers and marginally useful for actual product and design work.
It's disproportionately responsible for the worst things people say and
think about UXR. Doing so much of it just doesn't deliver enough
business value.

So many common forms of research questions live in the middle-range:

* How do users think/feel about X functionality/activity?
* What are the concerns or challenges with Y?
* Why are users using/not using Z feature?

Middle-range findings are usually not specific enough. They tend to be
too general and descriptive, even when a researcher does an amazing job
communicating. They're hard to turn into specific recommendations and
thus easy to poke holes in or ignore. They are most likely to trigger
the post-hoc bias, which invokes the stereotype that researchers work
for months only to tell us things we already know.

Of course, a talented researcher can mitigate some of these issues. But
there's still the structural disadvantage that comes from asking
mid-altitude questions that most cross-functional partners think they
already have the answers to anyway. All of this erodes the real and
perceived business value of even the best research. And we haven't even
gotten to the worst bit yet.
____

== link:https://www.infoq.com/news/2023/04/real-time-messaging-slack/[Real-time messaging architecture at Slack]

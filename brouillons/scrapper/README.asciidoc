= Ã‰crire un scrapper de site web en Ruby
Julien Kirch
v0.1, 2020-11-19
:article_lang: fr

Pour mon usage personnel, par exemple tÃ©lÃ©charger des sites ou des BDs pour les lire hors ligne sur une liseuse, ou pour rendre services Ã  des amis en extrayant des donnÃ©es structurÃ©es, j'ai souvent besoin d'Ã©crire des scrapper pour des sites web.

Il serait gÃ©nÃ©ralement possible d'utiliser un outil de scrapping gÃ©nÃ©rique et de le paramÃ©trer, ou de tÃ©lÃ©charger l'intÃ©gralitÃ© du site avec un outil comme link:https://www.httrack.com[HTTrack] puis de traiter les fichiers en local, mais un scrapper maison est souvent plus efficace car on peut facilement ne tÃ©lÃ©charger que les contenu qui nous intÃ©ressent, et extraire Ã  la volÃ©e les donnÃ©es dont on a besoin.

Une fois qu'on a une base de code qui fonctionne et qu'on maÃ®trise bien, le modifier pour un nouveau usage se fait facilement.
C'est un des cas oÃ¹ copier / coller un existant et le rÃ©Ã©diter pour chaque nouveau besoin est plus efficace que d'essayer d'avoir une solution gÃ©nÃ©rique qu'on aurait "`seulement`" Ã  configurer.

Cet article se propose de vous guider pas Ã  pas jusqu'Ã  avoir une premiÃ¨re version complÃ¨te, dans l'idÃ©e de vous rendre autonome.

Par ailleurs, la conception d'un scrapper permet de creuser quelques sujets intÃ©ressantes, naturellement un peu de HTML et de HTTP mais aussi de stockage de donnÃ©es.

Avertissement{nbsp}: de nombreux sites indiquent dans leur conditions d'utilisation qu'il n'est pas autorisÃ© de les scrapper.
Je ne sais pas si ces messages ont une valeur lÃ©gale, mais cet article n'est pas un encouragement Ã  scrapper ces sites.

== Le scrapper Ã  dÃ©velopper

Je vais ici dÃ©velopper un scrapper qui fait de l'archivage de site web.
Son objectif est d'obtenir une copie locale d'un site qui soit navigable en ouvrant les fichiers dans un navigateur.

Pour les fichiers et le parcours du site, le scrapper parcourra le HTML gÃ©nÃ©rÃ© cÃ´tÃ© serveur sans exÃ©cuter le JavaScript, et ne sera donc pas compatible avec des sites gÃ©nÃ©rÃ©s cÃ´tÃ© client.

Il est possible d'Ã©crire en Ruby des scrapper qui savent exÃ©cuter du JavaScript, par exemple avec des outils comme linkhttps://github.com/YusukeIwaki/puppeteer-ruby[Puppeteer Ruby] qui peut piloter une instance de Chrome, mais cela demande plus de travail et j'en ai rarement eu besoin quand j'ai fais du scrapping, je ne traiterai donc pas ce sujet ici.

Comme l'objectif est d'obtenir un code facile Ã  modifier plutÃ´t que facile Ã  Ã©tendre, le code sera placÃ© dans un fichier unique est dans un style procÃ©dural, c'est-Ã -dire en Ã©vitant de structurer le code Ã  l'aide de classe mais en utilisant uniquement des mÃ©thodes.

== TÃ©lÃ©charger une seule page

Pour commencer je vais tÃ©lÃ©charger uniquement le contenu HTML d'une page dont l'adresse est spÃ©cifiÃ©e dans le code.

Pour le tÃ©lÃ©chargement j'utilise la classe `Net::HTTP` fournie par la bibliothÃ¨que standard Ruby.
Certes il existe de nombreuses librairies externes fournissant des fonctionnalitÃ©s plus Ã©voluÃ©es et/ou des API plus simple, mais pour les besoins de scrapping `Net::HTTP` suffit amplement, et il intÃ©ressant de la connaÃ®tre un peu car elle est utilisÃ©e par dÃ©faut par de nombreuses librairies.
Pour un scrapper, la seule mÃ©thode Ã  connaÃ®tre est `Net::HTTP#get_response` qui fait une requÃªte `GET` Ã  partir d'une URL.

La classe `URI::HTTP` de la bibliothÃ¨que standard Ruby implÃ©mente les diffÃ©rents standards mais elle est assez stricte.
Cela signifie qu'elle n'apprÃ©cie pas toujours qu'on prenne des libertÃ© avec les standard.
Les navigateurs sont eux plutÃ´t tolÃ©rants, et par consÃ©quent les sites webs ne sont pas incitÃ©s Ã  suivre les standard au pied de la lettre.
Cela signifie que pour du scrapper, mieux un parseur d'URL tolÃ©rant, et pour cela je vais utiliser la bibliothÃ¨que link:https://github.com/sporkmonger/addressable[Addressable].

Voici donc le code{nbsp}:

.scrapper.rb
[source, ruby]
----
include::scrapper_1.rb[]
----

L'exÃ©cution du script devrait crÃ©er en local un fichier `index.html` qui contient le source de la page.
La page tÃ©lÃ©chargÃ©e Ã  link:http://example.com[example.com] n'utilisant ni image ni CSS externe, la version locale est auto-portante (en tous cas au moment oÃ¹ j'Ã©cris cet article).

== ConfigurabilitÃ©{nbsp}? Non, merci{nbsp}!

Pour un outil classique, la deuxiÃ¨me Ã©tape serait de rendre le script configurable, en permettant par exemple de lui passer l'URL d'entrÃ©e du site ou le rÃ©pertoire cible en paramÃ¨tre.

Mais rappelez vous que mon objectif n'est pas d'avoir un script configurable mais un script facilement Ã©ditable.
Avoir une URL en dur dans une constante en dÃ©but de fichier fait donc trÃ¨s bien l'affaire.

== TÃ©lÃ©charger une page et ses dÃ©pendances

Ã€ la place, la deuxiÃ¨me Ã©tape va consister Ã  tÃ©lÃ©charger une page et ses dÃ©pendances externes{nbsp}: images, feuilles de style et fichiers JavaScript.

Je vais prendre pour cible le site du magazine link:https://queue.acm.org[ACM Queue] qui est un magazine publiant des articles d'ingÃ©nierie logicielle.

== URLS et noms de fichiers

Mais d'abord il me faut parler des URLs et des noms de fichiers.

link:http://www.faqs.org/rfcs/rfc1738.html[Il y a trÃ¨s longtemps], les URLs ne pouvaient quasiment utiliser que des caractÃ¨res alphanumÃ©riques.
Les dinosaures rÃ©gnaient sur le monde et la vie Ã©tait simple.

DÃ©sormais on peut avoir des URLs avec des loutres comme link:https://emojipedia.org/emoji/ğŸ¦¦/[https://emojipedia.org/emoji/ğŸ¦¦/].

Dans un scrapper qui archive un site chaque contenu est sauvegardÃ©.
Si cette sauvegarde est faite dans une base de donnÃ©es classique, comme par exemple une base SQL, vous pouvez stocker les URLs dans un champ de texte, mÃªme si elles contiennent des loutres.

Si la base de donnÃ©e utilisÃ©e par le stockage est un systÃ¨me de fichier, les choses peuvent Ãªtre moins simple.

Les systÃ¨mes d'exploitation peuvent Ãªtre peu contraignants sur les formats de noms de fichiers (par exemple avec le systÃ¨me de fichiers ext4 souvent utilisÃ© sous Linux tous les caractÃ¨res sont autorisÃ©s Ã  part `NULL` et `/`).

Mais en pratique vous n'avez peut-Ãªtre pas envie d'avoir sur votre disque dur uÌˆÌŠÍÍ‰ÌÍ–Í‡Ì¥Ì«nÌ…Ì¾Ì¿Ì¢ÌŸÍ…Í–ÌºÌ—Ì¥Ì±Ì¬ Ì‘Í€Ì¬ fÍ†Í§Ò‰ÌºÌªÍšÌ©Ì­Ì­Ì™iÌ½ÌŠÌ‘ÌµÌ¤ÌŸÍšÌ³Ì ÌŸÌ£Ì¬cÍŠÍ¤Í ÍÌ³Ì˜ÌŸÌ¼hÌÌ’Í¯ÍªÍ˜Ì«Ì«Í…iÌ…Í¥Ì€Í Ì¦Í‰ÌÌ©Ì Ì«Ì²eÌ’ÌµÍšÌ˜rÌ’Í‘Í¦ÍÌ¹ÌÍ”ÌªÍ‰Ì™ ÍŒÍ¢Í‰Ì²Í“Ì˜Í…qÌŒÍŠÌÌšÌ¶ÍˆÌºÍˆÌ«ÌœÍÍ‰Í‰uÍÍ¤Í¢Ì«Ì¤Í–Ì¼Ì®ÌiÌ‚Í‚Ì‡Ì½Ì•Ì°Ì±Í•Í”Ì®Ì—Ì² Í„ÍŒÒ‰Ì­Í…ÍÌªsÌ„ÌÍ‹Í¡Ì£Í•Í‡Í•Ì¯Ì—Ì—Ì­Í‚Ì¨Í™Ì­Ì³Ì¼Ì–'ÌˆÌÌ´Ì©Ì¥Ì­Ì¤Ì«Ì–aÍ©Ì†Ì½Í›ÍÌ³Ì™ÍÌ¯pÍªÍ—Ì‚ÍŸÌÌ®Ì¦Ì¹Í‡Ì¥pÍ¤Í­ÍŸÍˆÍ™Í“Ì»eÍ†Í¬Ì†Í­Ì•Í…ÌªlÍƒÍ«Ì•ÍˆÌ©ÌœlÌŠÌ“Í£Ì´ÌÌŸÌ¼Í•Í™Ì®Ì¤ÌºeÍ’Ì”Í›ÌšÌ·Í‡Ì°Í™ Ì†Í¡Ì­Ì»Ì°Í‡Ì–cÌ½ÍªÍ„Ì¨Ì¬Ì–Ì¥Ì–oÌ’Ì‚Ì¾Í¤Ì¢ÍˆÌ²Ì­ÍˆÌŸÌ«Ì­mÍ§Í¡Ì­Í‰Ì©Í”ÍÌ¼Ì³Í–mÍ†Ì”Ì´ÍÌ™Ì³ÌŸÌ–eÌŠÍ‚Í¬ÌŠÌ¨Í‡Ì²Í…Ì»Ì Í ÍƒÌ’ÌƒÌ¨Í•Í”Í•Ì¹Ì¼ï¿½Í§Í©Ì›ÍÌœÍ‡Ì¹Ì»Ì°ÍšÌ¹ï¿½Í«Ì†Ì‘ÍÌ­Ì—Í“Í–Ì¤Ì–Ì¬aÌÌ¡ÌªÍÌª

Cela signifie qu'il faut trouver une opÃ©ration permettant de transformer les URLs en nom de fichier acceptables.

On pourrait vouloir supprimer les caractÃ¨res qu'on veut Ã©viter en ne conservant que les caractÃ¨res compatibles.
C'est ce qui est est fait dans certains CMS pour transformer des noms de fichiers en URLs.

L'inconvÃ©nient de cette mÃ©thode est qu'elle supprime des informations (les caractÃ¨res non acceptables), et qu'en faisait Ã§a deux URLs diffÃ©rentes peuvent Ãªtre transformer en un nom de fichier identique.

`https://emojipedia.org/emoji/ğŸ¦¦/` et `https://emojipedia.org/emoji/ğŸ¦”/` auraient ainsi le mÃªme nom de fichier.

L'opÃ©ration de transformation doit donc avoir les caractÃ©ristiques suivantes{nbsp}:

- une mÃªme URL doit toujours about au mÃªme nom de fichier (pour Ã©viter d'avoir des doublons)
- un nom de fichier doit correspondre Ã  une seule URL (pour Ã©viter le cas dÃ©crit plus haut)

En mathÃ©matique, ce type de transformation est appelÃ© link:https://fr.wikipedia.org/wiki/Bijection[bijection].

Une maniÃ¨re de mettre en place cette transformation est d'utiliser un dictionnaire, c'est-Ã -dire une source de donnÃ©es qui permette de stocker la correspondance entre URL en nom de fichier.
C'est un des usages des bases de donnÃ©es SQL{nbsp}:
on peut ainsi crÃ©er une table `URL` avec une colonne qui contient l'URL originale avec une clÃ© d'unicitÃ©, et une colonne contenant un incrÃ©ment automatique (aussi appelÃ© sÃ©quence).
Lorsqu'on veut rÃ©cupÃ©rer le nom de fichier correspondant Ã  une URL, on commence par vÃ©rifier si cette URL est dÃ©jÃ  dans la table, et sinon on l'insÃ¨re.
La valeur de la colonne contenant l'incrÃ©ment automatique fournit alors des identifiant adaptÃ©s Ã  des fichiers, on aurait ainsi des fichiers `1`, `2`â€¦
C'est comme si le nom de fichier Ã©tait une clÃ© Ã©trangÃ¨re vis-Ã -vis de cette table.

Sans utiliser de base de donnÃ©e externe, on pourrait gÃ©rer ce dictionnaire dans un fichier, par exemple un fichier JSON, ou mÃªme un fichier textuel contenant une URL par ligne en utilisant le numÃ©ro de ligne comme identifiant pour le fichier.

L'approche dictionnaire Ã  un inconvÃ©nient{nbsp}: elle demande de passer par une source de donnÃ©e supplÃ©mentaire qui n'est pas un des fichiers du site.

Une autre solution est de trouver une transformation qui rÃ©ponde aux deux caractÃ©ristiques dÃ©crite ci-dessous et qui se passe de dictionnaire.

Encoder les URL au format link:https://fr.wikipedia.org/wiki/Base64[base64], souvent utilisÃ© sur le web pour encoder des resources binaire dans un contenu textuel est un bon candidat car cela ne repose pas sur un dictionnaire et rÃ©pond aux deux caractÃ©ristiques, mais il a deux inconvÃ©nients qui empÃªchent de s'en servir :

- Il utilise le caractÃ¨re '/' qui n'est pas lÃ©gal dans des noms de fichiers.
- Il utilise des majuscules et des minuscules, alors que certains systÃ¨mes de fichiers (notablement celui de macOS) ne font pas cette distinction.

Un format d'encodage qui n'a pas ces inconvÃ©nients est d'utiliser les numÃ©ros des caractÃ¨res composant l'URL (dont le nom officiel est link:https://fr.wikipedia.org/wiki/Point_de_code[point de code]). Comme il s'agit d'une suite de valeurs numÃ©riques cela convient bien Ã  des noms de fichiers. On peut mÃªme utiliser la reprÃ©sentation hexadÃ©cimale de ces nombres pour avoir des noms de fichiers moins longs.

En Ruby, la mÃ©thode `String#ord` permet de rÃ©cupÃ©rer le point de code d'un caractÃ¨re, voici comment faire la transformation{nbsp}:

[source, ruby]
----
url = 'https://emojipedia.org/emoji/ğŸ¦¦/'
decimal_codepoints = url.chars.map{ |c| c.ord }
hexadecimal_codepoints = decimal_codepoints.map{ |c| c.to_s(16) }
file_name = hexadecimal_codepoints.join('-')
----

Ce qui nous donne `68-74-74-70-73-3a-2f-2f-65-6d-6f-6a-69-70-65-64-69-61-2e-6f-72-67-2f-65-6d-6f-6a-69-2f-1f9a6-2f`.

Il est nÃ©cessaire de mettre des tirets entre les nombres pour pouvoir distinguer les valeurs successives, car elles n'ont pas toutes la mÃªme longueur.

== TÃ©lÃ©charger une page et ses dÃ©pendances (pour de vrai)

Maintenant qu'on sait comment transformer une URL en nom de fichier, on peut s'intÃ©resser au tÃ©lÃ©chargement proprement dit.

Pour tÃ©lÃ©charger les dÃ©pendances externes d'une page, le mÃ©canisme est le suivant{nbsp}:

. Identifier la liste des dÃ©pendances
. Pour chaque dÃ©pendance
.. Si elle n'a pas dÃ©jÃ  Ã©tÃ© tÃ©lÃ©chargÃ©e, la tÃ©lÃ©charger et la stocker sous le nom calculÃ© en utilisant le code ci-dessus
.. Modifier le HTML de la page pour remplacer l'URL de la dÃ©pendance par le chemin du fichier

Pour identifier la liste des dÃ©pendances, il faut parcourir le HTML.
Pour cela la bibliothÃ¨que Ruby la plus utilisÃ©e est link:https://nokogiri.org[Nokogiri], elle sait parser du HTML et du XML et fournit ensuite une interface permettant de requÃªter et de modifier le contenu.

La premiÃ¨re Ã©tape est de parser le contenu du HTML que l'on reÃ§oit, et de prÃ©parer le fait que la sauvegarde va se faire dans un sous-rÃ©pertoire pour Ã©viter de mÃ©langer le site avec le scrapper.

Comme l'objectif est de scrapper le magazine ACM Queue, j'ai aussi remplacÃ© l'URL.

.scrapper.rb
[source, ruby]
----
include::scrapper_2.rb[]
----

Mais, quand on essaie d'ouvrir
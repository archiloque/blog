= Ã‰crire un scraper de site web en Ruby
Julien Kirch
v0.1, 2020-11-27
:article_lang: fr
:source-highlighter: pygments
:article_description: Pour lire des BDs ou faire des livres Ã©lectroniques Ã  partir de sites
:article_image: 2794.jpg
:ignore_files: 2794.jpg, Gemfile.lock, header.xml, scraper_01.rb, scraper_02.rb, scraper_03.rb, scraper_04.rb, scraper_05.rb, scraper_06.rb, scraper_07.rb, scraper_08.rb, scraper_09.rb, scraper_10.rb

Pour mon usage personnel, par exemple tÃ©lÃ©charger des sites ou des BDs pour les lire hors ligne sur une liseuse, ou pour rendre services Ã  des amis en extrayant des donnÃ©es structurÃ©es, j'ai souvent besoin d'Ã©crire des scraper pour des sites web.

Il serait gÃ©nÃ©ralement possible d'utiliser un outil de scraping gÃ©nÃ©rique et de le paramÃ©trer, ou de tÃ©lÃ©charger l'intÃ©gralitÃ© du site avec un outil comme link:https://www.httrack.com[HTTrack] puis de traiter les fichiers en local, mais un scraper maison est souvent plus efficace car on peut facilement ne tÃ©lÃ©charger que les contenu qui nous intÃ©ressent, et extraire Ã  la volÃ©e les donnÃ©es dont on a besoin.

Une fois qu'on a une base de code qui fonctionne et qu'on maÃ®trise bien, le modifier pour un nouveau usage se fait facilement.
C'est un des cas oÃ¹ copier / coller un existant et le rÃ©Ã©diter pour chaque nouveau besoin est plus efficace que d'essayer d'avoir une solution gÃ©nÃ©rique qu'on aurait "`seulement`" Ã  configurer.

Cet article se propose de vous guider pas Ã  pas jusqu'Ã  avoir une premiÃ¨re version complÃ¨te, dans l'idÃ©e de vous rendre autonome.

Par ailleurs, la conception d'un scraper permet de creuser quelques sujets intÃ©ressantes, naturellement un peu de HTML et de HTTP mais aussi de stockage de donnÃ©es.

Avertissement{nbsp}: de nombreux sites indiquent dans leur conditions d'utilisation qu'il n'est pas autorisÃ© de les scraper.
Je ne sais pas si ces messages ont une valeur lÃ©gale, mais cet article n'est pas un encouragement Ã  scraper ces sites.

== Le scraper Ã  dÃ©velopper

Je vais ici dÃ©velopper un scraper qui fait de l'archivage de site web.
Son objectif est d'obtenir une copie locale d'un site qui soit navigable en ouvrant les fichiers dans un navigateur.

Pour les fichiers et le parcours du site, le scraper parcourra le HTML gÃ©nÃ©rÃ© cÃ´tÃ© serveur sans exÃ©cuter le JavaScript, et ne sera donc pas compatible avec des sites gÃ©nÃ©rÃ©s cÃ´tÃ© client.

Il est possible d'Ã©crire en Ruby des scraper qui savent exÃ©cuter du JavaScript, par exemple avec des outils comme link:https://github.com/YusukeIwaki/puppeteer-ruby[Puppeteer Ruby] qui peut piloter une instance de Chrome, mais cela demande plus de travail et j'en ai rarement eu besoin quand j'ai fais du scraping, je ne traiterai donc pas ce sujet ici.

Comme l'objectif est d'obtenir un code facile Ã  modifier plutÃ´t que facile Ã  Ã©tendre, le code sera placÃ© dans un fichier unique est dans un style procÃ©dural, c'est-Ã -dire en Ã©vitant de structurer le code Ã  l'aide de classe mais en utilisant uniquement des mÃ©thodes.

== TÃ©lÃ©charger une seule page

Pour commencer je vais tÃ©lÃ©charger uniquement le contenu HTML d'une page dont l'adresse est spÃ©cifiÃ©e dans le code.

Pour le tÃ©lÃ©chargement j'utilise la classe `Net::HTTP` fournie par la bibliothÃ¨que standard Ruby.
Certes il existe de nombreuses librairies externes fournissant des fonctionnalitÃ©s plus Ã©voluÃ©es et/ou des API plus simple, mais pour les besoins de scraping `Net::HTTP` suffit gÃ©nÃ©ralement, et il intÃ©ressant de la connaÃ®tre un peu car elle est utilisÃ©e par dÃ©faut par de nombreuses librairies.
Pour un scraper minimal, la seule mÃ©thode Ã  connaÃ®tre est `Net::HTTP#get` qui fait une requÃªte `GET` Ã  partir d'une URL et qui renvoie le contenu.

La classe `URI::HTTP` de la bibliothÃ¨que standard Ruby implÃ©mente les diffÃ©rents standards mais elle est assez stricte.
Cela signifie qu'elle n'apprÃ©cie pas toujours qu'on prenne des libertÃ© avec les standard.
Les navigateurs sont eux plutÃ´t tolÃ©rants, et par consÃ©quent les sites  ne sont pas incitÃ©s Ã  suivre les standard au pied de la lettre.
Cela signifie que pour du scraper, mieux un parser d'URL tolÃ©rant, et pour cela je vais utiliser la bibliothÃ¨que link:https://github.com/sporkmonger/addressable[Addressable].

Voici donc le code{nbsp}:

[source, ruby]
----
include::scraper_01.rb[]
----

L'exÃ©cution du script devrait crÃ©er en local un fichier `index.html` qui contient le source de la page.
La page tÃ©lÃ©chargÃ©e Ã  link:http://example.com[example.com] n'utilisant ni image ni CSS externe, la version locale est auto-portante (en tous cas au moment oÃ¹ j'Ã©cris cet article).

== ConfigurabilitÃ©{nbsp}? Non, merci{nbsp}!

Pour un outil classique, la deuxiÃ¨me Ã©tape serait de rendre le script configurable, en permettant par exemple de lui passer l'URL d'entrÃ©e du site ou le rÃ©pertoire cible en paramÃ¨tre.

Mais rappelez vous que mon objectif n'est pas d'avoir un script configurable mais un script facilement Ã©ditable.
Avoir une URL en dur dans une constante en dÃ©but de fichier fait donc trÃ¨s bien l'affaire.

== TÃ©lÃ©charger une page et ses dÃ©pendances

Ã€ la place, la deuxiÃ¨me Ã©tape va consister Ã  tÃ©lÃ©charger une page et ses dÃ©pendances externes{nbsp}: images, feuilles de style et fichiers JavaScript.

Je vais prendre pour cible le site du magazine link:https://queue.acm.org[ACM Queue] qui est un magazine publiant des articles d'ingÃ©nierie logicielle.

== URLS et noms de fichiers

Mais d'abord il me faut parler des URLs et des noms de fichiers.

link:http://www.faqs.org/rfcs/rfc1738.html[Il y a trÃ¨s longtemps], les URLs ne pouvaient quasiment utiliser que des caractÃ¨res alphanumÃ©riques.
Les dinosaures rÃ©gnaient sur le monde et la vie Ã©tait simple.

DÃ©sormais on peut avoir des URLs avec des loutres comme link:https://emojipedia.org/emoji/ğŸ¦¦/[https://emojipedia.org/emoji/ğŸ¦¦/].

Dans un scraper qui archive un site chaque contenu est sauvegardÃ©.
Si cette sauvegarde est faite dans une base de donnÃ©es classique, comme par exemple une base SQL, vous pouvez stocker les URLs dans un champ de texte, mÃªme si elles contiennent des loutres.

Si la base de donnÃ©e utilisÃ©e par le stockage est un systÃ¨me de fichier, les choses peuvent Ãªtre moins simple.

Les systÃ¨mes d'exploitation peuvent Ãªtre peu contraignants sur les formats de noms de fichiers (par exemple avec le systÃ¨me de fichiers ext4 souvent utilisÃ© sous Linux tous les caractÃ¨res sont autorisÃ©s Ã  part `NULL` et `/`).

Mais en pratique vous n'avez peut-Ãªtre pas envie d'avoir sur votre disque dur uÌˆÌŠÍÍ‰ÌÍ–Í‡Ì¥Ì«nÌ…Ì¾Ì¿Ì¢ÌŸÍ…Í–ÌºÌ—Ì¥Ì±Ì¬ Ì‘Í€Ì¬ fÍ†Í§Ò‰ÌºÌªÍšÌ©Ì­Ì­Ì™iÌ½ÌŠÌ‘ÌµÌ¤ÌŸÍšÌ³Ì ÌŸÌ£Ì¬cÍŠÍ¤Í ÍÌ³Ì˜ÌŸÌ¼hÌÌ’Í¯ÍªÍ˜Ì«Ì«Í…iÌ…Í¥Ì€Í Ì¦Í‰ÌÌ©Ì Ì«Ì²eÌ’ÌµÍšÌ˜rÌ’Í‘Í¦ÍÌ¹ÌÍ”ÌªÍ‰Ì™ ÍŒÍ¢Í‰Ì²Í“Ì˜Í…qÌŒÍŠÌÌšÌ¶ÍˆÌºÍˆÌ«ÌœÍÍ‰Í‰uÍÍ¤Í¢Ì«Ì¤Í–Ì¼Ì®ÌiÌ‚Í‚Ì‡Ì½Ì•Ì°Ì±Í•Í”Ì®Ì—Ì² Í„ÍŒÒ‰Ì­Í…ÍÌªsÌ„ÌÍ‹Í¡Ì£Í•Í‡Í•Ì¯Ì—Ì—Ì­Í‚Ì¨Í™Ì­Ì³Ì¼Ì–'ÌˆÌÌ´Ì©Ì¥Ì­Ì¤Ì«Ì–aÍ©Ì†Ì½Í›ÍÌ³Ì™ÍÌ¯pÍªÍ—Ì‚ÍŸÌÌ®Ì¦Ì¹Í‡Ì¥pÍ¤Í­ÍŸÍˆÍ™Í“Ì»eÍ†Í¬Ì†Í­Ì•Í…ÌªlÍƒÍ«Ì•ÍˆÌ©ÌœlÌŠÌ“Í£Ì´ÌÌŸÌ¼Í•Í™Ì®Ì¤ÌºeÍ’Ì”Í›ÌšÌ·Í‡Ì°Í™ Ì†Í¡Ì­Ì»Ì°Í‡Ì–cÌ½ÍªÍ„Ì¨Ì¬Ì–Ì¥Ì–oÌ’Ì‚Ì¾Í¤Ì¢ÍˆÌ²Ì­ÍˆÌŸÌ«Ì­mÍ§Í¡Ì­Í‰Ì©Í”ÍÌ¼Ì³Í–mÍ†Ì”Ì´ÍÌ™Ì³ÌŸÌ–eÌŠÍ‚Í¬ÌŠÌ¨Í‡Ì²Í…Ì»Ì Í ÍƒÌ’ÌƒÌ¨Í•Í”Í•Ì¹Ì¼ï¿½Í§Í©Ì›ÍÌœÍ‡Ì¹Ì»Ì°ÍšÌ¹ï¿½Í«Ì†Ì‘ÍÌ­Ì—Í“Í–Ì¤Ì–Ì¬aÌÌ¡ÌªÍÌª

Cela signifie qu'il faut trouver une opÃ©ration permettant de transformer les URLs en nom de fichier acceptables.

On pourrait vouloir supprimer les caractÃ¨res qu'on veut Ã©viter en ne conservant que les caractÃ¨res compatibles.
C'est ce qui est est fait dans certains CMS pour transformer des noms de fichiers en URLs.

L'inconvÃ©nient de cette mÃ©thode est qu'elle supprime des informations (les caractÃ¨res non acceptables), et qu'en faisait Ã§a deux URLs diffÃ©rentes peuvent Ãªtre transformÃ©es en un nom de fichier identique.

`https://emojipedia.org/emoji/ğŸ¦¦/` et `https://emojipedia.org/emoji/ğŸ¦”/` auraient ainsi le mÃªme nom de fichier.

L'opÃ©ration de transformation doit donc avoir les caractÃ©ristiques suivantes{nbsp}:

- une mÃªme URL doit toujours about au mÃªme nom de fichier (pour Ã©viter d'avoir des doublons)
- un nom de fichier doit correspondre Ã  une seule URL (pour Ã©viter le cas dÃ©crit plus haut)

En mathÃ©matique, ce type de transformation est appelÃ© link:https://fr.wikipedia.org/wiki/Bijection[bijection].

Une mÃ©thode pour transformer une URL en nom de fichier serait d'utiliser les numÃ©ros des caractÃ¨res composant l'URL (dont le nom officiel est link:https://fr.wikipedia.org/wiki/Point_de_code[point de code]).
Comme il s'agit d'une suite de valeurs numÃ©riques cela convient bien Ã  des noms de fichiers. Malheureusement les noms de fichiers rÃ©sultants sont plutÃ´t longs (`https://emojipedia.org/emoji/ğŸ¦¦/` est transformÃ© en `68-74-74-70-73-3a-2f-2f-65-6d-6f-6a-69-70-65-64-69-61-2e-6f-72-67-2f-65-6d-6f-6a-69-2f-1f9a6-2f`) ce qui n'est pas compatible avec tous les systÃ¨mes de fichiers.

Si vous voulez suivre cette approche une solution possible est de sÃ©parer la chaÃ®ne ainsi calculÃ©e en plusieurs tronÃ§ons, par exemple par blocs de 10 valeurs, et que chaque tronÃ§on intermÃ©diaire corresponde Ã  un niveau de rÃ©pertoire (dans notre exemple `68-74-74-70-73-3a-2f-2f-65-6d/6f-6a-69-70-65-64-69-61-2e-6f/72-67-2f-65-6d-6f-6a-69-2f-1f9a6/2f`).
L'avantage de cette approche est que la transformation entre l'URL et le nom de fichier dÃ©pend uniquement de l'URL, mais sa mise en Å“uvre commence Ã  Ãªtre compliquÃ©e.

Une autre maniÃ¨re de mettre en place cette transformation est d'utiliser un dictionnaire, c'est-Ã -dire une source de donnÃ©es qui permette de stocker la correspondance entre URL en nom de fichier.
C'est un des usages des bases de donnÃ©es SQL{nbsp}:
on peut ainsi crÃ©er une table `URL` avec une colonne qui contient l'URL originale avec une clÃ© d'unicitÃ©, et une colonne contenant un incrÃ©ment automatique (aussi appelÃ© sÃ©quence).
Lorsqu'on veut rÃ©cupÃ©rer le nom de fichier correspondant Ã  une URL, on commence par vÃ©rifier si cette URL est dÃ©jÃ  dans la table, et sinon on l'insÃ¨re.
La valeur de la colonne contenant l'incrÃ©ment automatique fournit alors des identifiant adaptÃ©s Ã  des fichiers, on aurait ainsi des fichiers `1`, `2`â€¦
C'est comme si le nom de fichier Ã©tait une clÃ© Ã©trangÃ¨re vis-Ã -vis de cette table.

Sans utiliser de systÃ¨me de base de donnÃ©e externe, on peut utiliser une structure en mÃ©moire, par exemple un `Hash` dont les clÃ©s seraient les URLs, ce qui assure l'unicitÃ©. Pour gÃ©nÃ©rer les valeurs de noms de fichiers, on peut utiliser le nombres d'entrÃ©es dans ce `Hash`, il s'agit d'une sorte d'incrÃ©ment naturel{nbsp}:

[source, ruby]
----
KNOWN_URLS = {}

url = 'https://emojipedia.org/emoji/ğŸ¦¦/'

if KNOWN_URLS.key?(url)
  file_name = KNOWN_URLS[absolute_url]
else
  file_name = KNOWN_URLS.length
  KNOWN_URLS[url] = file_name
end
----

Utiliser une structure en mÃ©moire convient trÃ¨s bien tant que le site Ã  scraper ne contient pas des millions de pages, au delÃ  il serait peut-Ãªtre intÃ©ressant d'utiliser une base de donnÃ©e externe.

== TÃ©lÃ©charger une page et ses dÃ©pendances (pour de vrai)

Maintenant qu'on sait comment transformer une URL en nom de fichier, on peut s'intÃ©resser au tÃ©lÃ©chargement proprement dit.

Pour tÃ©lÃ©charger les dÃ©pendances externes d'une page, le mÃ©canisme est le suivant{nbsp}:

. Identifier la liste des dÃ©pendances
. Pour chaque dÃ©pendance
.. Si elle n'a pas dÃ©jÃ  Ã©tÃ© tÃ©lÃ©chargÃ©e, la tÃ©lÃ©charger et la stocker sous le nom calculÃ© en utilisant la mÃ©thode dÃ©crite plus haut
.. Modifier le HTML de la page pour remplacer l'URL de la dÃ©pendance par le chemin du fichier

Pour identifier la liste des dÃ©pendances, il faut parcourir le HTML.
Pour cela la bibliothÃ¨que Ruby la plus utilisÃ©e est link:https://nokogiri.org[Nokogiri], elle sait parser du HTML et du XML et fournit ensuite une interface permettant de requÃªter et de modifier le contenu.

La premiÃ¨re Ã©tape est de parser le contenu du HTML que l'on reÃ§oit, et de prÃ©parer le fait que la sauvegarde va se faire dans un sous-rÃ©pertoire pour Ã©viter de mÃ©langer le site avec le scraper.

Comme l'objectif est de scraper le magazine ACM Queue, j'ai aussi remplacÃ© l'URL.

[source, ruby]
----
include::scraper_02.rb[]
----

Mais, quand on essaie d'ouvrir le fichier `download/index.html` rÃ©sultant{nbsp}:

image::blocked.png[]

Pas de panique{nbsp}: il s'agit d'un mÃ©canisme rudimentaire pour bloquer les robots.

Quand ce genre de choses arrive, il vaut mieux faire deux choses{nbsp}:

- utiliser une bibliothÃ¨que HTTP qui a un comportement plus proche de celui des navigateurs
- fournir une en-tÃªte link:https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent[User-Agent] pour accentuer la ressemblance avec un navigateur.

Si vous avez l'habitude d'utiliser une des bibliothÃ¨ques HTTP qui est faite pour des appels REST, elle risque d'avoir le mÃªme problÃ¨me, car une API REST n'a pas vocation Ã  bloquer les clients qui ne sont pas des navigateurs.

Je vais passer de `Net::HTTP` Ã  la bibliothÃ¨que link:https://github.com/taf2/curb[Curb] qui fournit une API Ruby au dessus de la bibliothÃ¨que libcurl, que vous connaissez peut-Ãªtre Ã  travers la commande `curl` utilisÃ©e pour faire des tÃ©lÃ©chargement depuis la ligne de commande.

[source, ruby, highlight=3..9]
----
include::scraper_03.rb[tag=download]
----

Et Ã§a fonctionne{nbsp}!

image::success_no_dependency.png[]

Le HTML est lÃ , mÃªme si pour le moment sans CSS ni image.

Si vous allez jeter un Å“il au fichier HTML, une petite surprise vous attend{nbsp}:

[source, xml]
----
include::header.xml[]
----

En effet le contenu n'est pas au format HTML mais au format XHTML.
Il s'agit Ã  peu prÃ¨s d'une version XML de HTML4, crÃ©Ã© par le W3C Ã  l'Ã©poque oÃ¹ le XML Ã©tait Ã  la monde.
Les deux avantages de cette approche Ã©taient de pouvoir utiliser des outils XML pour pouvoir publier des sites (ce qui permettait notamment d'avoir la mÃªme chaÃ®ne de publication pour les livres que pour le contenu web) et de pouvoir utiliser des parsers XML, plus performants que les parsers HTML (cela prend son sens quand on se souvient des tÃ©lÃ©phones mobiles disponibles dans les annÃ©es 2000).
La vague du XML sur le web a Ã©tÃ© stoppÃ©e lorsqu'Apple, Mozilla et Opera ont dÃ©cidÃ© de monter un putsch contre le W3C en crÃ©ant leur propre instance et leur propre format qui a abouti au HTML5.

Pour le scraper cela n'a aucune consÃ©quence car Nokogiri traite le HTML et le XHTML de la mÃªme maniÃ¨re.

Je commence par dÃ©placer le code de tÃ©lÃ©chargement dans une mÃ©thode sÃ©parÃ©e, car je vais en avoir besoin pour les ressources externes{nbsp}:

[source, ruby, indent=0]
----
include::scraper_04.rb[tag=fetch_content]
----

Ensuite je vais commencer par rÃ©cupÃ©rer les images.

Le code suit exactement les Ã©tapes dÃ©crites plus haut, pour commencer je ne vais pas tenter de factoriser de code.

Pour la recherche dans le HTML, Nokogiri propose une mÃ©thode `.css` qui link:https://nokogiri.org/tutorials/searching_a_xml_html_document.html[permet d'utiliser la syntaxe CSS]{nbsp}:

[source, ruby, indent=0]
----
include::scraper_04.rb[tag=scrape_images]
----

En lanÃ§ant le script, les fichiers images sont bien crÃ©es{nbsp}:

image::images_list_no_extension.png[]

Elles sont bien remplacÃ©es dans le HTML{nbsp}:

[source, html]
----
<a href="index.cfm"><img src="0"></a>
<br>
<a href="whyjoinacm.cfm"><img src="1" border="0" style="clear:both;"></a>
----

Et quand on visualise le fichier, les images s'affichent bien{nbsp}:

image::images_display.png[]

Par contre stocker les images dans des fichiers sans extension rend moins pratique l'exploration des rÃ©sultats si on veut par exemple parcourir les fichiers.

Il est possible de rÃ©cupÃ©rer les extension depuis les URLs et cela fonctionne plutÃ´t bien pour les images.
Malheureusement Ã§a n'est pas toujours le cas pour les fichiers gÃ©nÃ©rÃ©s dynamiquement comme les documents HTML ou XML.
Par consÃ©quent il vaut mieux s'appuyer sur la dÃ©claration de `Content-Type` qu'on trouve dans les entÃªte de rÃ©ponse.
La bibliothÃ¨que link:https://github.com/mime-types/ruby-mime-types[mime-types] permet de dÃ©duire l'extension qui correspond Ã  un certain type de contenu.

Je vais modifier la mÃ©thode `fetch_content` pour qu'elle renvoie l'extension Ã  utiliser en mÃªme temps que le contenu tÃ©lÃ©chargÃ©.

[source, ruby, indent=0, highlight=2;9..13]
----
include::scraper_05.rb[tag=fetch_content]
----

Et je vais m'en servir pour le nom des fichiers.

[source, ruby, indent=0, highlight=2..9]
----
include::scraper_05.rb[tag=scrape_images_internal]
----

Les fichiers avec extension sont du coup bien reconnu par le systÃ¨me qui affiche les miniatures.

image::images_list_with_extension.png[]

Reste Ã  faire de mÃªme pour les feuilles CSS et le JavaScript. Ce qui donne l'occasion de factoriser un peu le code en extrayant le code de traitement d'une ressource dans une mÃ©thode `scrape_resource`.

Par courtoisie pour Ã©viter de trop solliciter le serveur et potentiellement de se faire bloquer, je vais mettre une seconde d'attente aprÃ¨s chaque tÃ©lÃ©chargement.

[source, ruby, indent=0, highlight=25..26;36..42]
----
include::scraper_06.rb[tag=extract]
----

Et voilÃ {nbsp}:

image::first_page_with_resources.png[]

== TÃ©lÃ©charger le reste du site

Pour rÃ©cupÃ©rer l'ensemble du site, il va s'agir d'Ã©tendre le comportement actuel en l'Ã©tendant aux pages.

Lors du traitement d'une page, en plus des images, des feuilles de styles et des scripts, il va falloir chercher les liens.
Si la cible du lien n'est pas encore connu, il faut le tÃ©lÃ©charger, et s'il s'agit d'une page HTML, il faut la traiter Ã  son tour.

Ainsi de lien en lien, on devrait petit Ã  petit parcourir l'ensemble du contenu du site.

La conception du parcours des liens est intÃ©ressante.
Pour traiter une image il suffit de la tÃ©lÃ©charger et de la stocker sur disque en sauvegardant la relation entre son URL et le chemin du fichier oÃ¹ elle est stockÃ©e.

Mais lorsqu'un lien pointe vers une page HTML, cette page doit elle-mÃªme Ãªtre traitÃ©e en remplaÃ§ant les URLs qu'elle contient vers les chemins des fichiers, aprÃ¨s Ã©ventuellement les avoir sauvegardÃ©.

Si une page HTML A rÃ©fÃ©rence une page HTML B, pour traiter la page A j'ai besoin d'avoir le nom du fichier oÃ¹ B sera sauvegardÃ©e.
Si pour sauvegarder B je dois la traiter et donc parcourir ses liens, et que B a un lien vers une page C, j'ai besoin d'avoir le nom du fichier oÃ¹ C sera sauvegardÃ©e.
Si pour sauvegarder C je dois la traiterâ€¦

Bref on se retrouve avec un genre de fonctionnement rÃ©cursif, oÃ¹ on aurait besoin de parcourir l'ensemble du site une page aprÃ¨s l'autre pour pouvoir ensuite Ã  rebours sauvegarder les diffÃ©rentes pages{nbsp}: quand j'ai terminÃ© de traiter C je peux la sauvegarder, et donc terminer de traiter B et la sauvegarder, et donc traiter A et la sauvegarder.

Les choses sont en fait plus compliquÃ©es que cela car les liens entre pages forment souvent des cycles.
Par exemple si la page A est la page de sommaire du site, il y a des chances que toutes les pages vers lesquelles A pointe pointent Ã  leur tour vers elle.
Donc pour traiter A il y a besoin d'avoir traitÃ© B mais pour traiter B il y a besoin d'avoir traitÃ© A.

(â•¯Â°â–¡Â°)â•¯ï¸µ â”»â”â”»

La solution est une approche en deux passes en sÃ©parant d'une part ce qu'il est nÃ©cessaire de faire pour transformer les URLs des pages en lien vers des fichiers locaux, et d'autre part le fait de traiter ces pages{nbsp}:

. On commence par tÃ©lÃ©charger et sauvegarder les cible des diffÃ©rents liens _sans les traiter_, afin de vÃ©rifier si leur contenu est au format HTML et d'obtenir les chemins des fichiers correspondant, ce qui permet de finir de traiter A.
. Ensuite pour chaque cible de lien non encore traitÃ©, on relit le contenu qui a Ã©tÃ© sauvegardÃ© sur disque, on le traite (en tÃ©lÃ©chargeant possiblement les autres pages vers lesquelles il pointe), puis on Ã©crase le fichier avec la version traitÃ©e, c'est-Ã -dire avec les URLs des images, script, feuilles de styles et des liens qui pointent dÃ©sormais vers leurs fichiers.

En parcourant le site, le script conservera une liste de page HTML dÃ©jÃ  sauvegardÃ©es mais pas encore traitÃ©es.
Lors du traitement d'une page, on pourra dÃ©couvrir de nouveaux liens vers d'autres pages qui seront Ã  traiter Ã  leur tour et ainsi de suite.

Lorsque cette liste sera vide et donc toutes les pages traitÃ©es, cela signifiera que tout le site aura Ã©tÃ© parcouru.

La premiÃ¨re Ã©tape est d'extraire la partie de `scrape_resource` qui s'occupe de sauvegarder un contenu dans un fichier dans une mÃ©thode sÃ©parÃ©e, afin de pouvoir l'utiliser directement.

[source, ruby, indent=0, highlight=15;19..32]
----
include::scraper_07.rb[tag=scrape_resource]
----

Ensuite la mÃ©thode `scrape_link`, va me permettre de traiter les liens de la mÃªme maniÃ¨re que `scrape_resource` pour les autres types d'URL. `PAGES_TO_PROCESS` va contenir les pages qui restent Ã  traiter, j'en fait un `Set` pour avoir un dedoublonnage.

[source, ruby, indent=0]
----
include::scraper_07.rb[tag=scrape_link]
----

La mÃ©thode `scrape_link` est typiquement une mÃ©thode qui sera modifiÃ©e en fonction du contenu Ã  scraper.
On pourra par exemple exclure certaines URLs, exporter certaines informations dans un fichier ou une base de donnÃ©es sÃ©parÃ©es.

Les fonctionnements de `scrape_link` et celui de `scrape_resource` se ressemblent (vÃ©rifier si un contenu est dÃ©jÃ  dans la liste, et sinon le tÃ©lÃ©charger) et on pourrait Ãªtre tentÃ© de les factoriser.
Mais `scrape_link` est suffisamment diffÃ©rent pour que Ã§a ne soit pas une bonne idÃ©e car le mÃ©lange des deux perdrait en lisibilitÃ©.

Pour finir il manque la boucle de traitement des pages qui va Ãªtre chargÃ© de vider `PAGES_TO_PROCESS`.
Pour initialiser la boucle, `PAGES_TO_PROCESS` est remplis avec la page initiale qui doit auparavant Ãªtre sauvegardÃ©e.

[source, ruby, indent=0, highlight=1..14;30..37]
----
include::scraper_07.rb[tag=loop]
----

On peut vÃ©rifier le fonctionnement dans le log{nbsp}:

[source]
----
Supprime [download]
CrÃ©Ã© [download]
Traite [https://queue.acm.org] [download/0.html]
VÃ©rifie la ressource [https://queue.acm.org/img/acmqueue_logo.gif]
TÃ©lÃ©charge [https://queue.acm.org/img/acmqueue_logo.gif]
VÃ©rifie la ressource [https://queue.acm.org/img/logo_acm.gif]
TÃ©lÃ©charge [https://queue.acm.org/img/logo_acm.gif]
â€¦
Lien interne trouvÃ© [https://queue.acm.org/index.cfm]
Nouveau lien interne [https://queue.acm.org/index.cfm]
Nouvelle page Ã  traiter [https://queue.acm.org/index.cfm]
Lien interne trouvÃ© [https://queue.acm.org/whyjoinacm.cfm]
Nouveau lien interne [https://queue.acm.org/whyjoinacm.cfm]
Nouvelle page Ã  traiter [https://queue.acm.org/whyjoinacm.cfm]
Lien externe trouvÃ© [https://www.acm.org/joinacm2?ref=queue]
Lien interne trouvÃ© [https://queue.acm.org/whyjoinacm.cfm]
Lien interne trouvÃ© [https://queue.acm.org/index.cfm]
â€¦
Traite [https://queue.acm.org/index.cfm] [download/18.html]
VÃ©rifie la ressource [https://queue.acm.org/img/acmqueue_logo.gif]
VÃ©rifie la ressource [https://queue.acm.org/img/logo_acm.gif]
VÃ©rifie la ressource [https://queue.acm.org/confimg/Kode_Vicious2.jpg]
VÃ©rifie la ressource [https://queue.acm.org/confimg/2020VirtualWinterSummit125x125(002).png]
----

Si vous laissez le script tourner suffisamment longtemps, il devrait parcourir tout le site et l'archiver, et vous aurez sur votre disque dur une version navigable complÃ¨te.

== Les fragments

Le script fonctionne, on y est presque{nbsp}!
Il reste encore deux choses Ã  modifier.

La premiÃ¨re est la gestion des fragments dans les liens.

Pour le moment, on vÃ©rifie si une page est dÃ©jÃ  connue en vÃ©rifiant si son URL complÃ¨te existe dans `KNOWN_URLS`.
La consÃ©quence est que si une URL contient un fragment, par exemple `https://queue.acm.org/detail.cfm?id=2000516#content-comments`, elle est considÃ©rÃ©e comme une URL diffÃ©rente de `https://queue.acm.org/detail.cfm?id=2000516`, et sera donc archivÃ©e comme une page Ã  part. Suivant que les autres pages ont un lien avec ou sans fragment le lien aboutira donc Ã  des pages diffÃ©rentes.

La solution est donc de retirer les Ã©ventuels fragments des URLs avant de consulter le contenu de `KNOWN_URLS`.
Pour calculer une URL cible, il faut donc chercher le nom du fichier dans `KNOWN_URLS` en utilisant l'URL sans fragment, et ajouter le fragment au nom du fichier.
La modification concerne seulement la mÃ©thode `scrape_link`{nbsp}:

[source, ruby, indent=0, highlight=11..26]
----
include::scraper_08.rb[tag=scrape_link]
----

== Reprise en cas d'erreur

La derniÃ¨re choses Ã  modifier est de mettre en place une capacitÃ© de reprise en cas d'erreur.

Il arrive qu'un site ait des erreurs, ou que votre connexion internet plante, ou vous aurez peut-Ãªtre envie d'interrompre le process.

Et quand cela arrive, il serait dommage d'avoir Ã  recommencer le scraping depuis le dÃ©but.

Pour Ã©viter cela, il faut sauvegarder l'Ã©tat du scraping aprÃ¨s chaque Ã©tape, et lors du dÃ©marrage du script vÃ©rifier s'il existe un Ã©tat sauvegardÃ©, et dans ce cas le charger et repartir de ce point.

L'Ã©tat du scraping correspond Ã  deux ensemble de donnÃ©es{nbsp}:

- `KNOWN_URLS` qui est le dictionnaire qui fournit les noms de fichiers correspondant aux URLs
- `PAGES_TO_PROCESS` qui est la liste des pages qui restent Ã  traiter

Ã€ chaque fois qu'on modifie l'une de ces deux donnÃ©es il faut donc la sauvegarder.

Je vais sauvegarder `KNOWN_URLS` dans un fichier `known_urls.txt`, en utilisant une ligne par entrÃ©e{nbsp}:

[source]
----
https://queue.acm.org 0.html
https://queue.acm.org/img/acmqueue_logo.gif 1.gif
https://queue.acm.org/img/logo_acm.gif 2.gif
https://queue.acm.org/confimg/Kode_Vicious2.jpg 3.jpeg
https://queue.acm.org/confimg/2020VirtualWinterSummit125x125(002).png 4.png
https://queue.acm.org/app/2020_09-10_lrg.png 5.png
----

[source, ruby, indent=0, highlight=10]
----
include::scraper_09.rb[tag=save_content]
----

Et voici le code qui gÃ¨re le chargement au dÃ©marrage si le fichier est prÃ©sent{nbsp}:

[source, ruby, indent=0]
----
include::scraper_09.rb[tag=load_KNOWN_URLS]
----

`PAGES_TO_PROCESS` sera sauvegardÃ© dans un fichier `pages_to_process.txt`, en utilisant une ligne par entrÃ©e{nbsp}:

[source]
----
https://queue.acm.org 0.html
https://queue.acm.org/img/acmqueue_logo.gif 1.gif
https://queue.acm.org/img/logo_acm.gif 2.gif
https://queue.acm.org/confimg/Kode_Vicious2.jpg 3.jpeg
https://queue.acm.org/confimg/2020VirtualWinterSummit125x125(002).png 4.png
https://queue.acm.org/app/2020_09-10_lrg.png 5.png
----

[source, ruby, indent=0, highlight=4]
----
include::scraper_09.rb[tag=save_pages]
----

Et voici le code qui gÃ¨re le chargement au dÃ©marrage si le fichier est prÃ©sent, s'il en l'est pas on initialise le contenu avec le contenu du l'URL d'entrÃ©e du site{nbsp}:

[source, ruby, indent=0]
----
include::scraper_09.rb[tag=load_pages]
----

J'ai choisi de ne pas mettre de gestion d'erreur dans le script, par exemple une reconnexion automatique en cas d'erreur.
Si un site est particuliÃ¨rement instable il peut Ãªtre nÃ©cessaire d'en ajouter, mais par dÃ©faut je prÃ©fÃ¨re laisser remonter les exception et donc laisser le scraper s'arrÃªter pour me permettre de faire le choix moi-mÃªme.

== Conclusion

Et voilÃ {nbsp}: le scraper est terminÃ©{nbsp}!
J'espÃ¨re qu'il vous sera utile, si vous avez des questions vous pouvez me contacter et j'essaierai de vous aider.

Ci-dessous la version finale du script pour vous servir de base.

''''

[source, ruby]
----
include::scraper_10.rb[]
----

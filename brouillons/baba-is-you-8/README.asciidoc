[#baba-is-you-8]
ifeval::["{doctype}" == "book"]
= Part 8: learning to play
endif::[]
ifeval::["{doctype}" != "book"]
= Writing a "`Baba Is You`" solver, part 8: learning to play
endif::[]
:author: Julien Kirch
:revnumber: v0.1
:docdate: 2019-05-09
:article_lang: en
:ignore_files: 
:article_image: first-level1.png
ifndef::source-highlighter[]
:source-highlighter: pygments
:pygments-style: friendly
endif::[]
:article_description: Restarting
:figure-caption!:

ifeval::["{doctype}" == "book"]
Following the seventh part,
endif::[]
ifeval::["{doctype}" != "book"]
Following link:../baba-is-you-7/[the seventh part],
endif::[]
where I hit the wall of complexity, it's time to restart afresh.

The system I worked until now didn't really know how to play:
it knew the rules of the game, and tried to play all possible moves one after another until it found a solution.

This approach is easy to code, but doesn't work when the game has many possible moves.

In some games, a naive approach like this can be optimized by trying to eliminate some possibilities without exploring all the possible moves.
For example if it's possible to detect that a move makes the level's exit unreachable.

Unfortunately it's not the case for Baba is you, so the solution is learn the program how to play.

== Teach a machine to play

Learning the program to play means to implement a goal-oriented solver.

When a human play the game, they check the resources available on the board, then try to find an idea for a solution ("`Maybe I move this rock here so I can reach the flag there?`"), then try to find the mean to implement this idea.

If the idea can't be implemented, they look for another idea.

Sometimes solving the a requires sub-goals: if reaching the flag is not possible as is because a wall is blocking the path, is it possible to change the rule first so Baba can move across the wall?

The target is to create a system which rely on the same process.

Compared to the naive approach, cutting the solving into sub-goals should decrease the number of states to explore because the possible movements for each sub-goals is low enough to be manageable.
For example trying to reach a specific cell without changing any rule.

This means this approach should have a better scalability for complex levels.

Its drawback is that managing all the cases requires much more code.